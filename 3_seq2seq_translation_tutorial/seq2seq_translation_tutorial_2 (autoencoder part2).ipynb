{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:41.812692Z",
     "start_time": "2024-03-26T13:38:41.145879Z"
    }
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\n",
    "**Author**: [Sean Robertson](https://github.com/spro)\n",
    "\n",
    "This is the third and final tutorial on doing \"NLP From Scratch\", where we\n",
    "write our own classes and functions to preprocess the data to do our NLP\n",
    "modeling tasks. We hope after you complete this tutorial that you'll proceed to\n",
    "learn how `torchtext` can handle much of this preprocessing for you in the\n",
    "three tutorials immediately following this one.\n",
    "\n",
    "In this project we will be teaching a neural network to translate from\n",
    "French to English.\n",
    "\n",
    "```sh\n",
    "[KEY: > input, = target, < output]\n",
    "\n",
    "> il est en train de peindre un tableau .\n",
    "= he is painting a picture .\n",
    "< he is painting a picture .\n",
    "\n",
    "> pourquoi ne pas essayer ce vin delicieux ?\n",
    "= why not try that delicious wine ?\n",
    "< why not try that delicious wine ?\n",
    "\n",
    "> elle n est pas poete mais romanciere .\n",
    "= she is not a poet but a novelist .\n",
    "< she not not a poet but a novelist .\n",
    "\n",
    "> vous etes trop maigre .\n",
    "= you re too skinny .\n",
    "< you re all alone .\n",
    "```\n",
    "... to varying degrees of success.\n",
    "\n",
    "This is made possible by the simple but powerful idea of the [sequence\n",
    "to sequence network](https://arxiv.org/abs/1409.3215)_, in which two\n",
    "recurrent neural networks work together to transform one sequence to\n",
    "another. An encoder network condenses an input sequence into a vector,\n",
    "and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
    "   :alt:\n",
    "\n",
    "To improve upon this model we'll use an [attention\n",
    "mechanism](https://arxiv.org/abs/1409.0473)_, which lets the decoder\n",
    "learn to focus over a specific range of the input sequence.\n",
    "\n",
    "**Recommended Reading:**\n",
    "\n",
    "I assume you have at least installed PyTorch, know Python, and\n",
    "understand Tensors:\n",
    "\n",
    "-  https://pytorch.org/ For installation instructions\n",
    "-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general\n",
    "-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview\n",
    "-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user\n",
    "\n",
    "\n",
    "It would also be useful to know about Sequence to Sequence networks and\n",
    "how they work:\n",
    "\n",
    "-  [Learning Phrase Representations using RNN Encoder-Decoder for\n",
    "   Statistical Machine Translation](https://arxiv.org/abs/1406.1078)_\n",
    "-  [Sequence to Sequence Learning with Neural\n",
    "   Networks](https://arxiv.org/abs/1409.3215)_\n",
    "-  [Neural Machine Translation by Jointly Learning to Align and\n",
    "   Translate](https://arxiv.org/abs/1409.0473)_\n",
    "-  [A Neural Conversational Model](https://arxiv.org/abs/1506.05869)_\n",
    "\n",
    "You will also find the previous tutorials on\n",
    ":doc:`/intermediate/char_rnn_classification_tutorial`\n",
    "and :doc:`/intermediate/char_rnn_generation_tutorial`\n",
    "helpful as those concepts are very similar to the Encoder and Decoder\n",
    "models, respectively.\n",
    "\n",
    "**Requirements**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:43.304562Z",
     "start_time": "2024-03-26T13:38:41.815252Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data files\n",
    "\n",
    "The data for this project is a set of many thousands of English to\n",
    "French translation pairs.\n",
    "\n",
    "[This question on Open Data Stack\n",
    "Exchange](https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages)_\n",
    "pointed me to the open translation site https://tatoeba.org/ which has\n",
    "downloads available at https://tatoeba.org/eng/downloads - and better\n",
    "yet, someone did the extra work of splitting language pairs into\n",
    "individual text files here: https://www.manythings.org/anki/\n",
    "\n",
    "The English to French pairs are too big to include in the repository, so\n",
    "download to ``data/eng-fra.txt`` before continuing. The file is a tab\n",
    "separated list of translation pairs:\n",
    "\n",
    "```sh\n",
    "I am cold.    J'ai froid.\n",
    "```\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Download the data from\n",
    "   [here](https://download.pytorch.org/tutorial/data.zip)\n",
    "   and extract it to the current directory.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:43.309258Z",
     "start_time": "2024-03-26T13:38:43.306758Z"
    }
   },
   "outputs": [],
   "source": [
    "#!wget https://download.pytorch.org/tutorial/data.zip\n",
    "#!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the character encoding used in the character-level RNN\n",
    "tutorials, we will be representing each word in a language as a one-hot\n",
    "vector, or giant vector of zeros except for a single one (at the index\n",
    "of the word). Compared to the dozens of characters that might exist in a\n",
    "language, there are many many more words, so the encoding vector is much\n",
    "larger. We will however cheat a bit and trim the data to only use a few\n",
    "thousand words per language.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/word-encoding.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a unique index per word to use as the inputs and targets of\n",
    "the networks later. To keep track of all this we will use a helper class\n",
    "called ``Lang`` which has word → index (``word2index``) and index → word\n",
    "(``index2word``) dictionaries, as well as a count of each word\n",
    "``word2count`` which will be used to replace rare words later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:43.318722Z",
     "start_time": "2024-03-26T13:38:43.313140Z"
    }
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are all in Unicode, to simplify we will turn Unicode\n",
    "characters to ASCII, make everything lowercase, and trim most\n",
    "punctuation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:43.325514Z",
     "start_time": "2024-03-26T13:38:43.321179Z"
    }
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split\n",
    "lines into pairs. The files are all English → Other Language, so if we\n",
    "want to translate from Other Language → English I added the ``reverse``\n",
    "flag to reverse the pairs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:43.332270Z",
     "start_time": "2024-03-26T13:38:43.327568Z"
    }
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are a *lot* of example sentences and we want to train\n",
    "something quickly, we'll trim the data set to only relatively short and\n",
    "simple sentences. Here the maximum length is 10 words (that includes\n",
    "ending punctuation) and we're filtering to sentences that translate to\n",
    "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
    "earlier).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:43.338582Z",
     "start_time": "2024-03-26T13:38:43.334380Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:43.343664Z",
     "start_time": "2024-03-26T13:38:43.340688Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Write new autoencoding dataset file\n",
    "\n",
    "# # Read the file and split into lines\n",
    "# lines = open('data/eng-fra.txt', encoding='utf-8').\\\n",
    "#     read().strip().split('\\n')\n",
    "\n",
    "# # Split every line into pairs and normalize\n",
    "# pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "# pairs = [[x[0], x[0]] for x in pairs]\n",
    "\n",
    "# new_lines = [\"\\t\".join(x) for x in pairs]\n",
    "\n",
    "# with open('data/eng-eng_2.txt', 'w') as f:\n",
    "#     for line in new_lines:\n",
    "#         f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:47.444145Z",
     "start_time": "2024-03-26T13:38:43.346269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n",
      "['ils font partie de nous', 'they re part of us']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Seq2Seq Model\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A [Sequence to Sequence network](https://arxiv.org/abs/1409.3215)_, or\n",
    "seq2seq network, or [Encoder Decoder\n",
    "network](https://arxiv.org/pdf/1406.1078v3.pdf)_, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
    "   :alt:\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence ``Je ne suis pas le chat noir`` → ``I am not the\n",
    "black cat``. Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. ``chat noir`` and ``black cat``. Because of the ``ne/pas``\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/encoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:47.453449Z",
     "start_time": "2024-03-26T13:38:47.449023Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Decoder\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and\n",
    "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
    "token, and the first hidden state is the context vector (the encoder's\n",
    "last hidden state).\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:47.462583Z",
     "start_time": "2024-03-26T13:38:47.455540Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to train and observe the results of this model, but to\n",
    "save space we'll be going straight for the gold and introducing the\n",
    "Attention Mechanism.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Decoder\n",
    "\n",
    "If only the context vector is passed between the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of\n",
    "the encoder's outputs for every step of the decoder's own outputs. First\n",
    "we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    ".. figure:: https://i.imgur.com/1152PYf.png\n",
    "   :alt:\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "Bahdanau attention, also known as additive attention, is a commonly used\n",
    "attention mechanism in sequence-to-sequence models, particularly in neural\n",
    "machine translation tasks. It was introduced by Bahdanau et al. in their\n",
    "paper titled [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)_.\n",
    "This attention mechanism employs a learned alignment model to compute attention\n",
    "scores between the encoder and decoder hidden states. It utilizes a feed-forward\n",
    "neural network to calculate alignment scores.\n",
    "\n",
    "However, there are alternative attention mechanisms available, such as Luong attention,\n",
    "which computes attention scores by taking the dot product between the decoder hidden\n",
    "state and the encoder hidden states. It does not involve the non-linear transformation\n",
    "used in Bahdanau attention.\n",
    "\n",
    "In this tutorial, we will be using Bahdanau attention. However, it would be a valuable\n",
    "exercise to explore modifying the attention mechanism to use Luong attention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:47.475237Z",
     "start_time": "2024-03-26T13:38:47.464547Z"
    }
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
    "  limitation by using a relative position approach. Read about \"local\n",
    "  attention\" in [Effective Approaches to Attention-based Neural Machine\n",
    "  Translation](https://arxiv.org/abs/1508.04025)_.</p></div>\n",
    "\n",
    "## Training\n",
    "\n",
    "### Preparing Training Data\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:47.483841Z",
     "start_time": "2024-03-26T13:38:47.477263Z"
    }
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but [when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf)_.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation -\n",
    "intuitively it has learned to represent the output grammar and can \"pick\n",
    "up\" the meaning once the teacher tells it the first few words, but it\n",
    "has not properly learned how to create the sentence from the translation\n",
    "in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "``teacher_forcing_ratio`` up to use more of it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:47.489946Z",
     "start_time": "2024-03-26T13:38:47.485667Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:47.495287Z",
     "start_time": "2024-03-26T13:38:47.491802Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:47.502634Z",
     "start_time": "2024-03-26T13:38:47.497143Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "``plot_losses`` saved while training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:47.507941Z",
     "start_time": "2024-03-26T13:38:47.504472Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:47.513821Z",
     "start_time": "2024-03-26T13:38:47.509802Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:47.519046Z",
     "start_time": "2024-03-26T13:38:47.515694Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
    "reasonable results.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>If you run this notebook you can train, interrupt the kernel,\n",
    "   evaluate, and continue training later. Comment out the lines where the\n",
    "   encoder and decoder are initialized and run ``trainIters`` again.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:51.719060Z",
     "start_time": "2024-03-26T13:38:47.520920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:51.727052Z",
     "start_time": "2024-03-26T13:38:51.721491Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "#         print(\"Encoder: \")\n",
    "#         print(\"Input Shape: \", input.shape)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "#         print(\"Embedded Shape: \", embedded.shape)\n",
    "        output, hidden = self.gru(embedded)\n",
    "#         print(\"Output Shape: \", output.shape)\n",
    "#         print(\"Hidden Shape: \", hidden.shape)\n",
    "#         print()\n",
    "        return output, hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:51.747009Z",
     "start_time": "2024-03-26T13:38:51.729856Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:51.789524Z",
     "start_time": "2024-03-26T13:38:51.749651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['je vais bien', 'i m ok'],\n",
       " ['ca va', 'i m ok'],\n",
       " ['je suis gras', 'i m fat'],\n",
       " ['je suis gros', 'i m fat'],\n",
       " ['je suis en forme', 'i m fit'],\n",
       " ['je suis touche !', 'i m hit !'],\n",
       " ['je suis touchee !', 'i m hit !'],\n",
       " ['je suis malade', 'i m ill'],\n",
       " ['je suis triste', 'i m sad'],\n",
       " ['je suis timide', 'i m shy'],\n",
       " ['je suis mouille', 'i m wet'],\n",
       " ['je suis mouillee', 'i m wet'],\n",
       " ['il est mouille', 'he s wet'],\n",
       " ['je suis gras', 'i am fat'],\n",
       " ['je suis revenu', 'i m back'],\n",
       " ['me revoila', 'i m back'],\n",
       " ['je suis chauve', 'i m bald'],\n",
       " ['je suis occupe', 'i m busy'],\n",
       " ['je suis occupee', 'i m busy'],\n",
       " ['je suis calme', 'i m calm'],\n",
       " ['j ai froid', 'i m cold'],\n",
       " ['j en ai fini', 'i m done'],\n",
       " ['tout va bien', 'i m fine'],\n",
       " ['je vais bien', 'i m fine'],\n",
       " ['ca va', 'i m fine'],\n",
       " ['je suis libre !', 'i m free !'],\n",
       " ['je suis libre', 'i m free'],\n",
       " ['je suis disponible', 'i m free'],\n",
       " ['je suis repu !', 'i m full'],\n",
       " ['je suis rassasie !', 'i m full'],\n",
       " ['je suis content', 'i m glad'],\n",
       " ['je suis chez moi', 'i m home'],\n",
       " ['je suis en retard', 'i m late'],\n",
       " ['je suis paresseux', 'i m lazy'],\n",
       " ['je suis faineant', 'i m lazy'],\n",
       " ['je suis paresseuse', 'i m lazy'],\n",
       " ['je suis faineante', 'i m lazy'],\n",
       " ['je vais bien', 'i m okay'],\n",
       " ['je me porte bien', 'i m okay'],\n",
       " ['je suis en securite', 'i m safe'],\n",
       " ['je suis malade', 'i m sick'],\n",
       " ['j en suis certain', 'i m sure'],\n",
       " ['je suis certain', 'i m sure'],\n",
       " ['j en suis sur', 'i m sure'],\n",
       " ['j en suis sure', 'i m sure'],\n",
       " ['je suis grande', 'i m tall'],\n",
       " ['je suis mince', 'i m thin'],\n",
       " ['je suis ordonne', 'i m tidy'],\n",
       " ['je suis ordonnee', 'i m tidy'],\n",
       " ['je suis laid', 'i m ugly'],\n",
       " ['je suis laide', 'i m ugly'],\n",
       " ['je suis faible', 'i m weak'],\n",
       " ['je vais bien', 'i m well'],\n",
       " ['je me porte bien', 'i m well'],\n",
       " ['il est malade', 'he is ill'],\n",
       " ['il est vieux', 'he is old'],\n",
       " ['il est dj', 'he s a dj'],\n",
       " ['il est bon', 'he s good'],\n",
       " ['il est paresseux', 'he s lazy'],\n",
       " ['il est riche', 'he s rich'],\n",
       " ['je suis occupe', 'i am busy'],\n",
       " ['je suis calme', 'i am calm'],\n",
       " ['j ai froid', 'i am cold'],\n",
       " ['je suis bon', 'i am good'],\n",
       " ['je suis ici', 'i am here'],\n",
       " ['je suis paresseux', 'i am lazy'],\n",
       " ['je suis faineant', 'i am lazy'],\n",
       " ['je suis paresseuse', 'i am lazy'],\n",
       " ['je suis faineante', 'i am lazy'],\n",
       " ['je vais bien', 'i am okay'],\n",
       " ['je suis malade', 'i am sick'],\n",
       " ['je suis sur', 'i am sure'],\n",
       " ['je suis certain', 'i am sure'],\n",
       " ['je suis faible', 'i am weak'],\n",
       " ['je suis flic', 'i m a cop'],\n",
       " ['je suis un homme', 'i m a man'],\n",
       " ['je suis seule', 'i m alone'],\n",
       " ['je suis seul', 'i m alone'],\n",
       " ['je suis arme', 'i m armed'],\n",
       " ['je suis armee', 'i m armed'],\n",
       " ['je suis reveille', 'i m awake'],\n",
       " ['je suis aveugle', 'i m blind'],\n",
       " ['je suis fauche', 'i m broke'],\n",
       " ['je suis fou', 'i m crazy'],\n",
       " ['je suis folle', 'i m crazy'],\n",
       " ['je suis gueri', 'i m cured'],\n",
       " ['je suis guerie', 'i m cured'],\n",
       " ['je suis saoul', 'i m drunk'],\n",
       " ['je suis soul', 'i m drunk'],\n",
       " ['je suis ivre', 'i m drunk'],\n",
       " ['je me meurs', 'i m dying'],\n",
       " ['je suis en avance', 'i m early'],\n",
       " ['je suis en premier', 'i m first'],\n",
       " ['je suis difficile', 'i m fussy'],\n",
       " ['je suis tatillon', 'i m fussy'],\n",
       " ['je suis tatillonne', 'i m fussy'],\n",
       " ['je pars maintenant', 'i m going'],\n",
       " ['je me tire', 'i m going'],\n",
       " ['j y vais', 'i m going'],\n",
       " ['je pars', 'i m going'],\n",
       " ['je suis loyal', 'i m loyal'],\n",
       " ['je suis loyale', 'i m loyal'],\n",
       " ['je suis veinard', 'i m lucky'],\n",
       " ['je suis veinarde', 'i m lucky'],\n",
       " ['j ai du pot', 'i m lucky'],\n",
       " ['je suis chanceux', 'i m lucky'],\n",
       " ['je suis chanceuse', 'i m lucky'],\n",
       " ['je suis en train de mentir', 'i m lying'],\n",
       " ['je suis tranquille', 'i m quiet'],\n",
       " ['je suis prete !', 'i m ready !'],\n",
       " ['je suis pret !', 'i m ready !'],\n",
       " ['je suis pret', 'i m ready'],\n",
       " ['j ai raison', 'i m right'],\n",
       " ['je suis sobre', 'i m sober'],\n",
       " ['excuse moi', 'i m sorry'],\n",
       " ['desole', 'i m sorry'],\n",
       " ['desole !', 'i m sorry'],\n",
       " ['je suis desole', 'i m sorry'],\n",
       " ['je suis desolee', 'i m sorry'],\n",
       " ['je suis coincee', 'i m stuck'],\n",
       " ['je suis timide', 'i m timid'],\n",
       " ['je suis fatigue !', 'i m tired'],\n",
       " ['je suis dur', 'i m tough'],\n",
       " ['je suis dure', 'i m tough'],\n",
       " ['je suis dur a cuire', 'i m tough'],\n",
       " ['je suis dure a cuire', 'i m tough'],\n",
       " ['je suis a toi', 'i m yours'],\n",
       " ['je suis a vous', 'i m yours'],\n",
       " ['elle est chaude', 'she s hot'],\n",
       " ['elle est tres attirante', 'she s hot'],\n",
       " ['nous avons chaud', 'we re hot'],\n",
       " ['nous sommes tristes', 'we re sad'],\n",
       " ['nous sommes timides', 'we re shy'],\n",
       " ['il est dj', 'he is a dj'],\n",
       " ['il a a faire', 'he is busy'],\n",
       " ['il est ici !', 'he is here !'],\n",
       " ['il est gentil', 'he is kind'],\n",
       " ['il est en retard', 'he is late'],\n",
       " ['il est faineant', 'he is lazy'],\n",
       " ['il est paresseux', 'he is lazy'],\n",
       " ['il est pauvre', 'he is poor'],\n",
       " ['il est malade', 'he is sick'],\n",
       " ['il est suisse', 'he s swiss'],\n",
       " ['il est helvete', 'he s swiss'],\n",
       " ['il est ruine', 'he s broke'],\n",
       " ['il est fauche', 'he s broke'],\n",
       " ['il est ivre', 'he s drunk'],\n",
       " ['il est soul', 'he s drunk'],\n",
       " ['il est intelligent', 'he s smart'],\n",
       " ['je suis un homme', 'i am a man'],\n",
       " ['je suis humain', 'i am human'],\n",
       " ['je suis pret', 'i am ready'],\n",
       " ['j ai raison', 'i am right'],\n",
       " ['je suis desole', 'i am sorry'],\n",
       " ['je suis desolee', 'i am sorry'],\n",
       " ['je suis fatigue !', 'i am tired'],\n",
       " ['je suis creve', 'i am tired'],\n",
       " ['je suis francais', 'i m french'],\n",
       " ['je suis coreen', 'i m korean'],\n",
       " ['je suis un heros', 'i m a hero'],\n",
       " ['je suis un menteur', 'i m a liar'],\n",
       " ['je cuis !', 'i m baking !'],\n",
       " ['je vais mieux', 'i m better'],\n",
       " ['je paie', 'i m buying'],\n",
       " ['c est moi qui paie', 'i m buying'],\n",
       " ['je suis grassouillet', 'i m chubby'],\n",
       " ['je suis grassouillette', 'i m chubby'],\n",
       " ['je mange', 'i m eating'],\n",
       " ['je suis connu', 'i m famous'],\n",
       " ['je suis connue', 'i m famous'],\n",
       " ['je suis plus rapide', 'i m faster'],\n",
       " ['je suis flasque', 'i m flabby'],\n",
       " ['je suis cupide', 'i m greedy'],\n",
       " ['je suis gourmand', 'i m greedy'],\n",
       " ['je suis gourmande', 'i m greedy'],\n",
       " ['je me cache', 'i m hiding'],\n",
       " ['je suis honnete', 'i m honest'],\n",
       " ['je suis humble', 'i m humble'],\n",
       " ['j ai faim !', 'i m hungry !'],\n",
       " ['j ai faim !', 'i m hungry'],\n",
       " ['je suis immunise', 'i m immune'],\n",
       " ['je suis immunisee', 'i m immune'],\n",
       " ['je suis au lit', 'i m in bed'],\n",
       " ['je suis alite', 'i m in bed'],\n",
       " ['je suis alitee', 'i m in bed'],\n",
       " ['je rigole !', 'i m joking'],\n",
       " ['je suis saoul', 'i m loaded'],\n",
       " ['je me sens seul', 'i m lonely'],\n",
       " ['je me sens seule', 'i m lonely'],\n",
       " ['je suis en train de perdre', 'i m losing'],\n",
       " ['je suis en train de demenager', 'i m moving'],\n",
       " ['je suis normal', 'i m normal'],\n",
       " ['je suis normale', 'i m normal'],\n",
       " ['c est moi qui paie', 'i m paying'],\n",
       " ['je suis en train de payer', 'i m paying'],\n",
       " ['je suis creve', 'i m pooped'],\n",
       " ['je suis repose', 'i m rested'],\n",
       " ['je suis reposee', 'i m rested'],\n",
       " ['je suis ruine', 'i m ruined'],\n",
       " ['je suis ruinee', 'i m ruined'],\n",
       " ['je suis remue', 'i m shaken'],\n",
       " ['je suis remuee', 'i m shaken'],\n",
       " ['je suis celibataire', 'i m single'],\n",
       " ['je suis maigrichon', 'i m skinny'],\n",
       " ['je suis maigrichonne', 'i m skinny'],\n",
       " ['je suis fatigue !', 'i m sleepy !'],\n",
       " ['j ai sommeil !', 'i m sleepy !'],\n",
       " ['je suis sournois', 'i m sneaky'],\n",
       " ['je suis sournoise', 'i m sneaky'],\n",
       " ['je suis strict', 'i m strict'],\n",
       " ['je suis stricte', 'i m strict'],\n",
       " ['je suis fort', 'i m strong'],\n",
       " ['je suis forte', 'i m strong'],\n",
       " ['j ai trente ans', 'i m thirty'],\n",
       " ['je suis affaiblie', 'i m wasted'],\n",
       " ['elle est vieille', 'she is old'],\n",
       " ['elle est occupee', 'she s busy'],\n",
       " ['elle est gentille', 'she s nice'],\n",
       " ['nous sommes des hommes', 'we are men'],\n",
       " ['nous sommes de retour', 'we re back'],\n",
       " ['nous sommes occupes', 'we re busy'],\n",
       " ['nous sommes occupees', 'we re busy'],\n",
       " ['nous avons fini', 'we re done'],\n",
       " ['nous avons termine', 'we re done'],\n",
       " ['nous en avons fini', 'we re done'],\n",
       " ['nous en avons termine', 'we re done'],\n",
       " ['nous sommes quittes', 'we re even'],\n",
       " ['nous allons bien', 'we re fine'],\n",
       " ['nous sommes ici', 'we re here'],\n",
       " ['nous sommes la', 'we re here'],\n",
       " ['nous sommes chez nous', 'we re home'],\n",
       " ['nous sommes en retard', 'we re late'],\n",
       " ['nous sommes perdus', 'we re lost'],\n",
       " ['nous sommes perdues', 'we re lost'],\n",
       " ['nous sommes riches', 'we re rich'],\n",
       " ['nous sommes en securite', 'we re safe'],\n",
       " ['on est foutu', 'we re sunk'],\n",
       " ['nous sommes foutus', 'we re sunk'],\n",
       " ['nous sommes faibles', 'we re weak'],\n",
       " ['tu es vilain', 'you re bad'],\n",
       " ['tu es grand', 'you re big'],\n",
       " ['tu es grande', 'you re big'],\n",
       " ['vous etes grand', 'you re big'],\n",
       " ['vous etes grande', 'you re big'],\n",
       " ['vous etes grands', 'you re big'],\n",
       " ['vous etes grandes', 'you re big'],\n",
       " ['t es marrante', 'you re fun'],\n",
       " ['t es marrant', 'you re fun'],\n",
       " ['tu es vieux', 'you re old'],\n",
       " ['tu es vieille', 'you re old'],\n",
       " ['vous etes vieux', 'you re old'],\n",
       " ['vous etes vieilles', 'you re old'],\n",
       " ['vous etes vieille', 'you re old'],\n",
       " ['tu es triste', 'you re sad'],\n",
       " ['vous etes triste', 'you re sad'],\n",
       " ['tu es timide', 'you re shy'],\n",
       " ['vous etes timide', 'you re shy'],\n",
       " ['il est ivre', 'he is drunk'],\n",
       " ['il est soul', 'he is drunk'],\n",
       " ['il a huit ans', 'he is eight'],\n",
       " ['il est hai', 'he is hated'],\n",
       " ['il est mechant', 'he is nasty'],\n",
       " ['il est intelligent', 'he is smart'],\n",
       " ['il est jeune', 'he is young'],\n",
       " ['c est un beau mec', 'he s a hunk'],\n",
       " ['c est un beau gosse', 'he s a hunk'],\n",
       " ['c est un pauvre type', 'he s a jerk'],\n",
       " ['c est un menteur', 'he s a liar'],\n",
       " ['c est un binoclard', 'he s a nerd'],\n",
       " ['c est un flemmard', 'he s a slob'],\n",
       " ['il est endormi', 'he s asleep'],\n",
       " ['il arrive', 'he s coming'],\n",
       " ['il est en train d arriver', 'he s coming'],\n",
       " ['il est en train de pleurer', 'he s crying'],\n",
       " ['il simule', 'he s faking'],\n",
       " ['il est blinde', 'he s loaded'],\n",
       " ['il est pete de thune', 'he s loaded'],\n",
       " ['il est plein aux as', 'he s loaded'],\n",
       " ['il a mon age', 'he s my age'],\n",
       " ['il n est pas chez lui', 'he s not in'],\n",
       " ['il n est pas a l interieur', 'he s not in'],\n",
       " ['il n est pas la', 'he s not in'],\n",
       " ['je suis francais', 'i am french'],\n",
       " ['je suis coreen', 'i am korean'],\n",
       " ['je suis cuisinier', 'i am a cook'],\n",
       " ['je suis un moine', 'i am a monk'],\n",
       " ['je vais mieux', 'i am better'],\n",
       " ['je suis mieux', 'i am better'],\n",
       " ['j arrive', 'i am coming'],\n",
       " ['j ai faim !', 'i am hungry'],\n",
       " ['je plaisante', 'i am joking'],\n",
       " ['je suis celibataire', 'i am single'],\n",
       " ['je suis plus grand', 'i am taller'],\n",
       " ['j ai egalement dix sept ans', 'i m too'],\n",
       " ['je suis finlandais', 'i m finnish'],\n",
       " ['je suis finlandaise', 'i m finnish'],\n",
       " ['je suis italien', 'i m italian'],\n",
       " ['je suis boulanger', 'i m a baker'],\n",
       " ['je suis boulangere', 'i m a baker'],\n",
       " ['je suis tout a fait pret', 'i m all set'],\n",
       " ['je suis tout a fait prete', 'i m all set'],\n",
       " ['j ai honte', 'i m ashamed'],\n",
       " ['je suis a la maison', 'i m at home'],\n",
       " ['je suis dans la maison', 'i m at home'],\n",
       " ['je suis perplexe', 'i m baffled'],\n",
       " ['je suis beni', 'i m blessed'],\n",
       " ['je suis benie', 'i m blessed'],\n",
       " ['je suis prudent', 'i m careful'],\n",
       " ['je suis prudente', 'i m careful'],\n",
       " ['je suis sur', 'i m certain'],\n",
       " ['je suis certain', 'i m certain'],\n",
       " ['j ai les foies', 'i m chicken'],\n",
       " ['j ai les chocottes', 'i m chicken'],\n",
       " ['j ai raison', 'i m correct'],\n",
       " ['je suis curieux', 'i m curious'],\n",
       " ['je suis curieuse', 'i m curious'],\n",
       " ['je suis en train de danser', 'i m dancing'],\n",
       " ['je suis au regime', 'i m dieting'],\n",
       " ['je suis en train de conduire', 'i m driving'],\n",
       " ['je conduis', 'i m driving'],\n",
       " ['je suis fiance', 'i m engaged'],\n",
       " ['je suis fiancee', 'i m engaged'],\n",
       " ['je suis excite', 'i m excited'],\n",
       " ['je suis excitee', 'i m excited'],\n",
       " ['je fais la diete', 'i m fasting'],\n",
       " ['je suis a la diete', 'i m fasting'],\n",
       " ['je suis meticuleux', 'i m finicky'],\n",
       " ['je suis meticuleuse', 'i m finicky'],\n",
       " ['je suis affole', 'i m frantic'],\n",
       " ['je suis affolee', 'i m frantic'],\n",
       " ['je suis furieux', 'i m furious'],\n",
       " ['je suis en bonne sante', 'i m healthy'],\n",
       " ['je fredonne', 'i m humming'],\n",
       " ['je suis veinard', 'i m in luck'],\n",
       " ['je suis veinarde', 'i m in luck'],\n",
       " ['je suis jalouse', 'i m jealous'],\n",
       " ['j ai la frousse', 'i m jittery'],\n",
       " ['je plaisante', 'i m kidding'],\n",
       " ['je rigole !', 'i m kidding'],\n",
       " ['je pars', 'i m leaving'],\n",
       " ['je suis marie', 'i m married'],\n",
       " ['je suis mariee', 'i m married'],\n",
       " ['je ne suis pas une idiote', 'i m no fool'],\n",
       " ['je ne suis pas un heros', 'i m no hero'],\n",
       " ['je ne suis pas un menteur', 'i m no liar'],\n",
       " ['je ne suis pas gros', 'i m not fat'],\n",
       " ['je ne suis pas fou', 'i m not mad'],\n",
       " ['je ne suis pas folle', 'i m not mad'],\n",
       " ['je ne suis pas vieux', 'i m not old'],\n",
       " ['je ne suis pas vieille', 'i m not old'],\n",
       " ['je ne suis pas triste', 'i m not sad'],\n",
       " ['je ne suis pas timide', 'i m not shy'],\n",
       " ['je suis en service', 'i m on duty'],\n",
       " ['je suis patiente', 'i m patient'],\n",
       " ['je suis patient', 'i m patient'],\n",
       " ['je suis populaire', 'i m popular'],\n",
       " ['je suis remonte', 'i m psyched'],\n",
       " ['je suis voyant', 'i m psychic'],\n",
       " ['je suis voyante', 'i m psychic'],\n",
       " ['je suis perplexe', 'i m puzzled'],\n",
       " ['je lis', 'i m reading'],\n",
       " ['je suis detendu', 'i m relaxed'],\n",
       " ['je suis detendue', 'i m relaxed'],\n",
       " ['je suis retraite', 'i m retired'],\n",
       " ['je suis retraitee', 'i m retired'],\n",
       " ['je suis pensionne', 'i m retired'],\n",
       " ['je suis pensionnee', 'i m retired'],\n",
       " ['je suis egoiste', 'i m selfish'],\n",
       " ['je suis serieux', 'i m serious'],\n",
       " ['je suis choque', 'i m shocked'],\n",
       " ['je suis choquee', 'i m shocked'],\n",
       " ['je suis sincere', 'i m sincere'],\n",
       " ['je suis bourre', 'i m sloshed'],\n",
       " ['je suis bourree', 'i m sloshed'],\n",
       " ['je suis tellement rassasie', 'i m so full'],\n",
       " ['je meurs de faim !', 'i m starved'],\n",
       " ['j ai l estomac dans les talons', 'i m starved'],\n",
       " ['j ai la dalle', 'i m starved'],\n",
       " ['j ai les crocs', 'i m starved'],\n",
       " ['je reste', 'i m staying'],\n",
       " ['je suis gave', 'i m stuffed'],\n",
       " ['je suis gavee', 'i m stuffed'],\n",
       " ['je suis sidere', 'i m stunned'],\n",
       " ['je suis sideree', 'i m stunned'],\n",
       " ['je suis en train de discuter', 'i m talking'],\n",
       " ['je taquine', 'i m teasing'],\n",
       " ['j ai soif', 'i m thirsty'],\n",
       " ['j en ai fini', 'i m through'],\n",
       " ['j en ai termine', 'i m through'],\n",
       " ['je suis trop gros', 'i m too fat'],\n",
       " ['je suis touche', 'i m touched'],\n",
       " ['je suis touchee', 'i m touched'],\n",
       " ['je suis mecontent', 'i m unhappy'],\n",
       " ['je suis mecontente', 'i m unhappy'],\n",
       " ['j ai la schcoumoune', 'i m unlucky'],\n",
       " ['je suis fortune', 'i m wealthy'],\n",
       " ['je suis fortunee', 'i m wealthy'],\n",
       " ['je gagne', 'i m winning'],\n",
       " ['je l emporte', 'i m winning'],\n",
       " ['je suis en train de travailler', 'i m working'],\n",
       " ['je me fais du souci', 'i m worried'],\n",
       " ['elle n a pas de charme', 'she is curt'],\n",
       " ['elle est morte', 'she is dead'],\n",
       " ['elle est gentille', 'she is kind'],\n",
       " ['elle est en retard', 'she is late'],\n",
       " ['c est une enfoiree', 'she s a dog'],\n",
       " ['c est un renard', 'she s a fox'],\n",
       " ['ils sont mauvais', 'they re bad'],\n",
       " ['elles sont mauvaises', 'they re bad'],\n",
       " ['ils sont vieux', 'they re old'],\n",
       " ['elles sont vieilles', 'they re old'],\n",
       " ['nous sommes quittes', 'we are even'],\n",
       " ['nous sommes a egalite', 'we are even'],\n",
       " ['nous sommes ici', 'we are here'],\n",
       " ['nous y sommes', 'we are here'],\n",
       " ['nous sommes en retard', 'we are late'],\n",
       " ['nous sommes seuls', 'we re alone'],\n",
       " ['nous sommes seules', 'we re alone'],\n",
       " ['nous sommes en colere', 'we re angry'],\n",
       " ['nous sommes armes', 'we re armed'],\n",
       " ['nous sommes armees', 'we re armed'],\n",
       " ['nous nous ennuyons', 'we re bored'],\n",
       " ['on s emmerde', 'we re bored'],\n",
       " ['nous sommes fauches', 'we re broke'],\n",
       " ['nous sommes fauchees', 'we re broke'],\n",
       " ['on est fauches', 'we re broke'],\n",
       " ['nous sommes en train de mourir', 'we re dying'],\n",
       " ['nous sommes en avance', 'we re early'],\n",
       " ['nous y allons', 'we re going'],\n",
       " ['nous sommes heureux', 'we re happy'],\n",
       " ['nous sommes pretes', 'we re ready'],\n",
       " ['nous sommes sauves', 'we re saved'],\n",
       " ['nous sommes sauvees', 'we re saved'],\n",
       " ['nous sommes intelligents', 'we re smart'],\n",
       " ['nous sommes intelligentes', 'we re smart'],\n",
       " ['nous sommes desoles', 'we re sorry'],\n",
       " ['nous sommes desolees', 'we re sorry'],\n",
       " ['nous sommes coinces', 'we re stuck'],\n",
       " ['nous sommes coincees', 'we re stuck'],\n",
       " ['nous sommes fatigues', 'we re tired'],\n",
       " ['nous sommes fatiguees', 'we re tired'],\n",
       " ['nous sommes jumeaux', 'we re twins'],\n",
       " ['nous sommes jumelles', 'we re twins'],\n",
       " ['tu es grand', 'you are big'],\n",
       " ['tu es grande', 'you are big'],\n",
       " ['vous etes grand', 'you are big'],\n",
       " ['vous etes grande', 'you are big'],\n",
       " ['vous etes grands', 'you are big'],\n",
       " ['vous etes grandes', 'you are big'],\n",
       " ['tu es fou', 'you are mad'],\n",
       " ['tu es de retour', 'you re back'],\n",
       " ['vous etes de retour', 'you re back'],\n",
       " ['t es sympa', 'you re cool'],\n",
       " ['t es decontracte', 'you re cool'],\n",
       " ['tu es juste', 'you re fair'],\n",
       " ['vous etes juste', 'you re fair'],\n",
       " ['vous etes justes', 'you re fair'],\n",
       " ['tu vas bien', 'you re fine'],\n",
       " ['tu es libre', 'you re free'],\n",
       " ['vous etes libres', 'you re free'],\n",
       " ['vous etes libre', 'you re free'],\n",
       " ['vous etes bon', 'you re good'],\n",
       " ['vous etes bonne', 'you re good'],\n",
       " ['vous etes bons', 'you re good'],\n",
       " ['vous etes bonnes', 'you re good'],\n",
       " ['tu es bon', 'you re good'],\n",
       " ['tu es bonne', 'you re good'],\n",
       " ['t es bon', 'you re good'],\n",
       " ['t es bonne', 'you re good'],\n",
       " ['tu es gentil', 'you re kind'],\n",
       " ['tu es gentille', 'you re kind'],\n",
       " ['tu es paresseux', 'you re lazy'],\n",
       " ['tu es paresseuse', 'you re lazy'],\n",
       " ['vous etes paresseux', 'you re lazy'],\n",
       " ['vous etes paresseuses', 'you re lazy'],\n",
       " ['vous etes paresseuse', 'you re lazy'],\n",
       " ['tu es perdu', 'you re lost'],\n",
       " ['tu es perdue', 'you re lost'],\n",
       " ['t es sympa', 'you re nice'],\n",
       " ['vous etes sympa', 'you re nice'],\n",
       " ['vous etes sympas', 'you re nice'],\n",
       " ['tu es fou !', 'you re nuts !'],\n",
       " ['t es dingue !', 'you re nuts !'],\n",
       " ['vous etes dingue !', 'you re nuts !'],\n",
       " ['vous etes dingues !', 'you re nuts !'],\n",
       " ['t es givre !', 'you re nuts !'],\n",
       " ['t es givree !', 'you re nuts !'],\n",
       " ['vous etes givre !', 'you re nuts !'],\n",
       " ['vous etes givree !', 'you re nuts !'],\n",
       " ['vous etes givres !', 'you re nuts !'],\n",
       " ['vous etes givrees !', 'you re nuts !'],\n",
       " ['tu es riche', 'you re rich'],\n",
       " ['vous etes riche', 'you re rich'],\n",
       " ['vous etes riches', 'you re rich'],\n",
       " ['tu es grossier', 'you re rude'],\n",
       " ['vous etes grossiers', 'you re rude'],\n",
       " ['vous etes grossiere', 'you re rude'],\n",
       " ['vous etes grossieres', 'you re rude'],\n",
       " ['tu es grossiere', 'you re rude'],\n",
       " ['tu es en securite', 'you re safe'],\n",
       " ['vous etes en securite', 'you re safe'],\n",
       " ['tu es sauf', 'you re safe'],\n",
       " ['vous etes saufs', 'you re safe'],\n",
       " ['vous etes sauf', 'you re safe'],\n",
       " ['vous etes sauve', 'you re safe'],\n",
       " ['vous etes sauves', 'you re safe'],\n",
       " ['tu es sauve', 'you re safe'],\n",
       " ['vous etes malade !', 'you re sick !'],\n",
       " ['tu es malade !', 'you re sick !'],\n",
       " ['vous etes malade !', 'you re sick !'],\n",
       " ['tu es mince', 'you re thin'],\n",
       " ['vous etes mince', 'you re thin'],\n",
       " ['vous etes minces', 'you re thin'],\n",
       " ['tu es faible', 'you re weak'],\n",
       " ['vous etes faible', 'you re weak'],\n",
       " ['vous etes faibles', 'you re weak'],\n",
       " ['c est un poete', 'he is a poet'],\n",
       " ['il est poete', 'he is a poet'],\n",
       " ['il est endormi', 'he is asleep'],\n",
       " ['il est excentrique', 'he is cranky'],\n",
       " ['il est en train de manger', 'he is eating'],\n",
       " ['il est heroique', 'he is heroic'],\n",
       " ['il n est pas chez lui', 'he is not in'],\n",
       " ['il n est pas a l interieur', 'he is not in'],\n",
       " ['il est anglais', 'he s english'],\n",
       " ['c est un bigot', 'he s a bigot'],\n",
       " ['c est un sectaire', 'he s a bigot'],\n",
       " ['c est un fanatique', 'he s a bigot'],\n",
       " ['c est un illumine', 'he s a bigot'],\n",
       " ['il souffre', 'he s in pain'],\n",
       " ['il est marie', 'he s married'],\n",
       " ['c est mon heros', 'he s my hero'],\n",
       " ['il est actuellement a l exterieur', 'he s out now'],\n",
       " ['il est si mignon !', 'he s so cute'],\n",
       " ['il est tellement mignon !', 'he s so cute'],\n",
       " ['je suis italien', 'i am italian'],\n",
       " ['j ai honte', 'i am ashamed'],\n",
       " ['je suis a la maison', 'i am at home'],\n",
       " ['je suis curieux', 'i am curious'],\n",
       " ['je suis mariee', 'i am married'],\n",
       " ['je suis tellement malade', 'i am so sick'],\n",
       " ['je suis si malade', 'i am so sick'],\n",
       " ['c est moi qui suis tellement malade', 'i am so sick'],\n",
       " ['c est moi qui suis si malade', 'i am so sick'],\n",
       " ['j ai soif', 'i am thirsty'],\n",
       " ['je suis en train de travailler', 'i am working'],\n",
       " ['je suis un couard', 'i m a coward'],\n",
       " ['je suis un lache', 'i m a coward'],\n",
       " ['je suis medecin', 'i m a doctor'],\n",
       " ['je suis toubib', 'i m a doctor'],\n",
       " ['je suis fermier', 'i m a farmer'],\n",
       " ['je suis agriculteur', 'i m a farmer'],\n",
       " ['je travaille dans ma ferme', 'i m a farmer'],\n",
       " ['je suis un agriculteur', 'i m a farmer'],\n",
       " ['je suis un puriste', 'i m a purist'],\n",
       " ['je suis drogue', 'i m addicted'],\n",
       " ['je suis accro', 'i m addicted'],\n",
       " ['j ai une assuetude', 'i m addicted'],\n",
       " ['j ai tout fini', 'i m all done'],\n",
       " ['j ai tout termine', 'i m all done'],\n",
       " ['je suis tout ouie', 'i m all ears'],\n",
       " ['je suis adulte', 'i m an adult'],\n",
       " ['je suis un agent', 'i m an agent'],\n",
       " ['je suis en train de saigner', 'i m bleeding'],\n",
       " ['je me melange les pinceaux', 'i m confused'],\n",
       " ['je suis creatif', 'i m creative'],\n",
       " ['je suis creative', 'i m creative'],\n",
       " ['je suis cultive', 'i m cultured'],\n",
       " ['je suis cultivee', 'i m cultured'],\n",
       " ['je suis divorce', 'i m divorced'],\n",
       " ['je suis divorcee', 'i m divorced'],\n",
       " ['je suis en train de me noyer', 'i m drowning'],\n",
       " ['j ai dix huit ans', 'i m eighteen'],\n",
       " ['je suis fidele', 'i m faithful'],\n",
       " ['je suis affame !', 'i m famished'],\n",
       " ['je suis intrepide', 'i m fearless'],\n",
       " ['je me bats', 'i m fighting'],\n",
       " ['j ai termine', 'i m finished'],\n",
       " ['j en ai termine', 'i m finished'],\n",
       " ['je suis libre maintenant', 'i m free now'],\n",
       " ['je suis gele', 'i m freezing'],\n",
       " ['je suis cloue', 'i m grounded'],\n",
       " ['je suis clouee', 'i m grounded'],\n",
       " ['je suis credule', 'i m gullible'],\n",
       " ['j ai le mal du pays', 'i m homesick'],\n",
       " ['j ai la gueule de bois', 'i m hungover'],\n",
       " ['je suis a paris', 'i m in paris'],\n",
       " ['je suis innocent', 'i m innocent'],\n",
       " ['je suis innocente', 'i m innocent'],\n",
       " ['je suis ingenu', 'i m innocent'],\n",
       " ['je suis ingenue', 'i m innocent'],\n",
       " ['je suis implique', 'i m involved'],\n",
       " ['je suis impliquee', 'i m involved'],\n",
       " ['je m en sors', 'i m managing'],\n",
       " ['je suis nouveau ici', 'i m new here'],\n",
       " ['je ne suis pas un rebelle', 'i m no rebel'],\n",
       " ['je ne suis pas un saint', 'i m no saint'],\n",
       " ['je ne suis pas une sainte', 'i m no saint'],\n",
       " ['je ne suis pas occupe', 'i m not busy'],\n",
       " ['je ne suis pas sourd', 'i m not deaf'],\n",
       " ['je ne suis pas sourde', 'i m not deaf'],\n",
       " ['je n ai pas fini', 'i m not done'],\n",
       " ['je n ai pas termine', 'i m not done'],\n",
       " ['je n en ai pas termine', 'i m not done'],\n",
       " ['je ne suis pas abruti', 'i m not dumb'],\n",
       " ['je ne suis pas abrutie', 'i m not dumb'],\n",
       " ['je ne suis pas mauvais', 'i m not evil'],\n",
       " ['je ne suis pas ici', 'i m not here'],\n",
       " ['je ne suis pas la', 'i m not here'],\n",
       " ['je ne suis pas chez moi', 'i m not home'],\n",
       " ['je ne suis pas a la maison', 'i m not home'],\n",
       " ['je ne suis pas blesse', 'i m not hurt'],\n",
       " ['je ne suis pas blessee', 'i m not hurt'],\n",
       " ['je ne suis pas mechant', 'i m not mean'],\n",
       " ['je ne suis pas mechante', 'i m not mean'],\n",
       " ['je ne suis pas riche', 'i m not rich'],\n",
       " ['je n en suis pas certain', 'i m not sure'],\n",
       " ['je n en suis pas certaine', 'i m not sure'],\n",
       " ['je ne suis pas grand', 'i m not tall'],\n",
       " ['je ne suis pas grande', 'i m not tall'],\n",
       " ['je ne suis pas laid', 'i m not ugly'],\n",
       " ['je ne suis pas laide', 'i m not ugly'],\n",
       " ['je ne vais pas bien', 'i m not well'],\n",
       " ['je ne suis pas en service', 'i m off duty'],\n",
       " ['je suis offense', 'i m offended'],\n",
       " ['je suis offensee', 'i m offended'],\n",
       " ['je suis indigne', 'i m outraged'],\n",
       " ['je suis indignee', 'i m outraged'],\n",
       " ['je suis puissant', 'i m powerful'],\n",
       " ['je suis puissante', 'i m powerful'],\n",
       " ['je suis pret', 'i m prepared'],\n",
       " ['je suis preparee', 'i m prepared'],\n",
       " ['je suis prepare', 'i m prepared'],\n",
       " ['je suis ponctuel', 'i m punctual'],\n",
       " ['je suis ponctuelle', 'i m punctual'],\n",
       " ['je suis rationnel', 'i m rational'],\n",
       " ['je suis rationnelle', 'i m rational'],\n",
       " ['je me suis amende', 'i m reformed'],\n",
       " ['je me suis amendee', 'i m reformed'],\n",
       " ['je suis fiable', 'i m reliable'],\n",
       " ['je ne tiens pas en place', 'i m restless'],\n",
       " ['je suis impitoyable', 'i m ruthless'],\n",
       " ['je tire', 'i m shooting'],\n",
       " ['je suis en train de dormir', 'i m sleeping'],\n",
       " ['je suis tellement desole !', 'i m so sorry'],\n",
       " ['je suis tellement desolee !', 'i m so sorry'],\n",
       " ['je suis si fatigue !', 'i m so tired !'],\n",
       " ['je suis tellement las !', 'i m so tired !'],\n",
       " ['je suis en train de parler', 'i m speaking'],\n",
       " ['je meurs de faim !', 'i m starving !'],\n",
       " ['je creve de faim !', 'i m starving'],\n",
       " ['j ai la fringale', 'i m starving'],\n",
       " ['je suis tetu', 'i m stubborn'],\n",
       " ['je suis tetue', 'i m stubborn'],\n",
       " ['je suis obstine', 'i m stubborn'],\n",
       " ['je suis obstinee', 'i m stubborn'],\n",
       " ['c est moi le patron', 'i m the boss'],\n",
       " ['c est moi la patronne', 'i m the boss'],\n",
       " ['je suis le patron', 'i m the boss'],\n",
       " ['je suis la patronne', 'i m the boss'],\n",
       " ['je reflechis', 'i m thinking'],\n",
       " ['je suis minutieux', 'i m thorough'],\n",
       " ['je suis minutieuse', 'i m thorough'],\n",
       " ['je suis consciencieux', 'i m thorough'],\n",
       " ['je suis consciencieuse', 'i m thorough'],\n",
       " ['je suis ravi', 'i m thrilled'],\n",
       " ['je suis ravie', 'i m thrilled'],\n",
       " ['je suis chatouilleux', 'i m ticklish'],\n",
       " ['je suis chatouilleuse', 'i m ticklish'],\n",
       " ['je suis trop occupe', 'i m too busy'],\n",
       " ['je suis trop occupee', 'i m too busy'],\n",
       " ['je suis trop affaire', 'i m too busy'],\n",
       " ['je suis trop affairee', 'i m too busy'],\n",
       " ['je dis la verite', 'i m truthful'],\n",
       " ['je suis impartial', 'i m unbiased'],\n",
       " ['je suis impartiale', 'i m unbiased'],\n",
       " ['je suis au dessus', 'i m upstairs'],\n",
       " ['je suis tres gros', 'i m very fat'],\n",
       " ['je suis tres gras', 'i m very fat'],\n",
       " ['je suis tres triste', 'i m very sad'],\n",
       " ['je suis tres timide', 'i m very shy'],\n",
       " ['je suis epuise', 'i m worn out'],\n",
       " ['c est un renard', 'she is a fox'],\n",
       " ['elle est chanceuse', 'she is lucky'],\n",
       " ['elle est chancarde', 'she is lucky'],\n",
       " ['elle est tranquille', 'she is quiet'],\n",
       " ['elle est tranchante', 'she is sharp'],\n",
       " ['elle est affutee', 'she is sharp'],\n",
       " ['elle a tort', 'she is wrong'],\n",
       " ['elle est jeune', 'she is young'],\n",
       " ['ils sont de retour', 'they re back'],\n",
       " ['elles sont de retour', 'they re back'],\n",
       " ['ce sont des garcons', 'they re boys'],\n",
       " ['ils sont froids', 'they re cold'],\n",
       " ['elles sont froides', 'they re cold'],\n",
       " ['ils sont sympa', 'they re cool'],\n",
       " ['elles sont sympa', 'they re cool'],\n",
       " ['ce sont des flics', 'they re cops'],\n",
       " ['ils sont mignons', 'they re cute'],\n",
       " ['elles sont mignonnes', 'they re cute'],\n",
       " ['ils sont decedes', 'they re dead'],\n",
       " ['elles sont decedees', 'they re dead'],\n",
       " ['ils ont termine', 'they re done'],\n",
       " ['elles ont termine', 'they re done'],\n",
       " ['ils sont libres', 'they re free'],\n",
       " ['elles sont libres', 'they re free'],\n",
       " ['ils sont partis', 'they re gone'],\n",
       " ['elles sont parties', 'they re gone'],\n",
       " ['il n y en a plus', 'they re gone'],\n",
       " ['ils sont la', 'they re here'],\n",
       " ['ils sont a moi', 'they re mine'],\n",
       " ['ils sont miens', 'they re mine'],\n",
       " ['ce sont les miens', 'they re mine'],\n",
       " ['ce sont les miennes', 'they re mine'],\n",
       " ['elles sont miennes', 'they re mine'],\n",
       " ['elles sont a moi', 'they re mine'],\n",
       " ['ils sont riches', 'they re rich'],\n",
       " ['elles sont riches', 'they re rich'],\n",
       " ['ils sont faibles', 'they re weak'],\n",
       " ['elles sont faibles', 'they re weak'],\n",
       " ['nous sommes arabes', 'we are arabs'],\n",
       " ['nous sommes heureux', 'we are happy'],\n",
       " ['nous sommes une equipe', 'we re a team'],\n",
       " ['nous sommes des adultes', 'we re adults'],\n",
       " ['nous allons tous bien', 'we re all ok'],\n",
       " ['nous sommes en guerre', 'we re at war'],\n",
       " ['notre point de vue est biaise', 'we re biased'],\n",
       " ['nous avons des prejuges', 'we re biased'],\n",
       " ['nous achetons', 'we re buying'],\n",
       " ['nous sommes fermes', 'we re closed'],\n",
       " ['nous venons', 'we re coming'],\n",
       " ['nous sortons ensemble', 'we re dating'],\n",
       " ['nous sommes condamnes', 'we re doomed'],\n",
       " ['nous nous cachons', 'we re hiding'],\n",
       " ['nous sommes a l interieur', 'we re inside'],\n",
       " ['nous plaisantons', 'we re joking'],\n",
       " ['nous perdons', 'we re losing'],\n",
       " ['nous sommes en train de perdre', 'we re losing'],\n",
       " ['nous sommes en train de bouger', 'we re moving'],\n",
       " ['nous sommes normaux', 'we re normal'],\n",
       " ['nous sommes normales', 'we re normal'],\n",
       " ['nous sommes en train de payer', 'we re paying'],\n",
       " ['nous sommes creves', 'we re pooped'],\n",
       " ['nous sommes crevees', 'we re pooped'],\n",
       " ['nous sommes ruines', 'we re ruined'],\n",
       " ['nous sommes ruinees', 'we re ruined'],\n",
       " ['nous sommes remues', 'we re shaken'],\n",
       " ['nous sommes remuees', 'we re shaken'],\n",
       " ['nous sommes sournois', 'we re sneaky'],\n",
       " ['nous sommes sournoises', 'we re sneaky'],\n",
       " ['nous sommes forts', 'we re strong'],\n",
       " ['nous sommes fortes', 'we re strong'],\n",
       " ['nous essayons', 'we re trying'],\n",
       " ['vous etes bon', 'you are good'],\n",
       " ['vous etes bonne', 'you are good'],\n",
       " ['vous etes bons', 'you are good'],\n",
       " ['vous etes bonnes', 'you are good'],\n",
       " ['tu es bon', 'you are good'],\n",
       " ['tu es bonne', 'you are good'],\n",
       " ['tu es en retard', 'you are late'],\n",
       " ['vous etes en retard', 'you are late'],\n",
       " ['vous etes riches', 'you are rich'],\n",
       " ['tu joues au chef', 'you re bossy'],\n",
       " ['tu fais le chef', 'you re bossy'],\n",
       " ['tu es fou', 'you re crazy'],\n",
       " ['tu es cruelle', 'you re cruel'],\n",
       " ['tu es cruel', 'you re cruel'],\n",
       " ['vous etes cruelle', 'you re cruel'],\n",
       " ['vous etes cruelles', 'you re cruel'],\n",
       " ['vous etes cruel', 'you re cruel'],\n",
       " ['vous etes cruels', 'you re cruel'],\n",
       " ['tu viens tot', 'you re early'],\n",
       " ['vous etes matinal', 'you re early'],\n",
       " ['vous etes matinale', 'you re early'],\n",
       " ['tu es matinal', 'you re early'],\n",
       " ['tu es matinale', 'you re early'],\n",
       " ['tu es en avance', 'you re early'],\n",
       " ['vous etes en avance', 'you re early'],\n",
       " ['tu es vire', 'you re fired'],\n",
       " ['tu es licencie', 'you re fired'],\n",
       " ['tu es en premier', 'you re first'],\n",
       " ['tu passes en premier', 'you re first'],\n",
       " ['vous etes en premier', 'you re first'],\n",
       " ['t es marrante', 'you re funny'],\n",
       " ['t es marrant', 'you re funny'],\n",
       " ['vous etes marrants', 'you re funny'],\n",
       " ['vous etes marrantes', 'you re funny'],\n",
       " ['vous etes marrant', 'you re funny'],\n",
       " ['vous etes marrante', 'you re funny'],\n",
       " ['tu es difficile', 'you re fussy'],\n",
       " ['tu fais des manieres', 'you re fussy'],\n",
       " ['vous faites des manieres', 'you re fussy'],\n",
       " ['tu es malpoli !', 'you re gross !'],\n",
       " ['tu mens !', 'you re lying !'],\n",
       " ['vous mentez !', 'you re lying !'],\n",
       " ['vous mentez', 'you re lying'],\n",
       " ['tu es lunatique', 'you re moody'],\n",
       " ['tu es naif', 'you re naive'],\n",
       " ['tu es naive', 'you re naive'],\n",
       " ['vous etes naif', 'you re naive'],\n",
       " ['vous etes naive', 'you re naive'],\n",
       " ['vous etes naifs', 'you re naive'],\n",
       " ['vous etes naives', 'you re naive'],\n",
       " ['tu es calme', 'you re quiet'],\n",
       " ['tu es tranquille', 'you re quiet'],\n",
       " ['tu as raison', 'you re right'],\n",
       " ['tu es idiot', 'you re silly'],\n",
       " ['vous etes idiot', 'you re silly'],\n",
       " ['vous etes idiote', 'you re silly'],\n",
       " ['tu es idiote', 'you re silly'],\n",
       " ['vous etes idiots', 'you re silly'],\n",
       " ['vous etes idiotes', 'you re silly'],\n",
       " ['t es plante', 'you re stuck'],\n",
       " ['t es plantee', 'you re stuck'],\n",
       " ['vous etes plantes', 'you re stuck'],\n",
       " ['vous etes plantees', 'you re stuck'],\n",
       " ['vous etes plantee', 'you re stuck'],\n",
       " ['vous etes plante', 'you re stuck'],\n",
       " ['tu es dure', 'you re tough'],\n",
       " ['tu es dur', 'you re tough'],\n",
       " ['vous etes dur', 'you re tough'],\n",
       " ['vous etes dure', 'you re tough'],\n",
       " ['vous etes durs', 'you re tough'],\n",
       " ['vous etes dures', 'you re tough'],\n",
       " ['tu es contrariee', 'you re upset'],\n",
       " ['tu es contrarie', 'you re upset'],\n",
       " ['tu es bizarre', 'you re weird'],\n",
       " ['vous etes bizarre', 'you re weird'],\n",
       " ['vous etes bizarres', 'you re weird'],\n",
       " ['tu es dans l erreur', 'you re wrong'],\n",
       " ['tu as tort', 'you re wrong'],\n",
       " ['tu es jeune', 'you re young'],\n",
       " ['vous etes jeune', 'you re young'],\n",
       " ['vous etes jeunes', 'you re young'],\n",
       " ['il est britannique', 'he is british'],\n",
       " ['il est anglais', 'he is english'],\n",
       " ['c est un voleur', 'he is a thief'],\n",
       " ['il est idiot', 'he is foolish'],\n",
       " ['c est mon patron', 'he is my boss'],\n",
       " ['il est mon genre !', 'he is my type !'],\n",
       " ['il n est pas idiot', 'he is no fool'],\n",
       " ['il n est pas fou', 'he is no fool'],\n",
       " ['il est actuellement sorti', 'he is out now'],\n",
       " ['il lit', 'he is reading'],\n",
       " ['il est en train de lire', 'he is reading'],\n",
       " ['il court', 'he is running'],\n",
       " ['il fait du patin', 'he is skating'],\n",
       " ['il patine', 'he is skating'],\n",
       " ['il est trop vieux', 'he is too old'],\n",
       " ['c est un rouspeteur', 'he s a grouch'],\n",
       " ['il est jesuite', 'he s a jesuit'],\n",
       " ['c est un aine', 'he s a senior'],\n",
       " ['c est un ancien', 'he s a senior'],\n",
       " ['c est un retraite', 'he s a senior'],\n",
       " ['c est un magnat', 'he s a tycoon'],\n",
       " ['il est accro', 'he s addicted'],\n",
       " ['il a une assuetude', 'he s addicted'],\n",
       " ['il est drogue', 'he s addicted'],\n",
       " ['il est adorable', 'he s adorable'],\n",
       " ['il est derriere moi', 'he s after me'],\n",
       " ['il est apres moi', 'he s after me'],\n",
       " ['il est apres mes fesses', 'he s after me'],\n",
       " ['il me poursuit', 'he s after me'],\n",
       " ['il est embetant', 'he s annoying'],\n",
       " ['il est fou', 'he s demented'],\n",
       " ['il est a tokyo', 'he s in tokyo'],\n",
       " ['il est innocent', 'he s innocent'],\n",
       " ['il est ingenu', 'he s innocent'],\n",
       " ['il est peu sur de lui', 'he s insecure'],\n",
       " ['il manque de confiance en lui', 'he s insecure'],\n",
       " ['il n est pas un saint', 'he s no saint'],\n",
       " ['ce n est pas un saint', 'he s no saint'],\n",
       " ['il n est pas a la maison', 'he s not home'],\n",
       " ['il n est pas malade', 'he s not sick'],\n",
       " ['il est indigne', 'he s outraged'],\n",
       " ['il est impitoyable', 'he s ruthless'],\n",
       " ['il est sans pitie', 'he s ruthless'],\n",
       " ['il est si jeune', 'he s so young'],\n",
       " ['il est tellement jeune', 'he s so young'],\n",
       " ['il etudie', 'he s studying'],\n",
       " ['il est en train d etudier', 'he s studying'],\n",
       " ['il est trop occupe', 'he s too busy'],\n",
       " ['il est trop lent', 'he s too slow'],\n",
       " ['il est tres malade', 'he s very ill'],\n",
       " ['il est fort malade', 'he s very ill'],\n",
       " ['il est ton fils', 'he s your son'],\n",
       " ['c est ton fils', 'he s your son'],\n",
       " ['c est votre fils', 'he s your son'],\n",
       " ['il est votre fils', 'he s your son'],\n",
       " ['je suis americain', 'i am american'],\n",
       " ['je suis americaine', 'i am american'],\n",
       " ['je suis japonais', 'i am japanese'],\n",
       " ['je suis musulman', 'i am a muslim'],\n",
       " ['je suis musulmane', 'i am a muslim'],\n",
       " ['je suis musulman', 'i am a muslim'],\n",
       " ['je suis un coureur', 'i am a runner'],\n",
       " ['je suis tout ouie', 'i am all ears'],\n",
       " ['je suis diabetique', 'i am diabetic'],\n",
       " ['je suis divorce', 'i am divorced'],\n",
       " ['je suis divorcee', 'i am divorced'],\n",
       " ['je suis a paris', 'i am in paris'],\n",
       " ['je suis nouveau ici', 'i am new here'],\n",
       " ['je suis nouveau ici', 'i am new here'],\n",
       " ['je ne suis pas sourd', 'i am not deaf'],\n",
       " ['je suis tellement desole !', 'i am so sorry'],\n",
       " ['je suis tellement desolee !', 'i am so sorry'],\n",
       " ['je suis tres triste', 'i am very sad'],\n",
       " ['je suis un patient', 'i m a patient'],\n",
       " ['je suis une patiente', 'i m a patient'],\n",
       " ['je suis etudiant', 'i m a student'],\n",
       " ['je suis professeur', 'i m a teacher'],\n",
       " ['je suis enseignante', 'i m a teacher'],\n",
       " ['je m adapte', 'i m adaptable'],\n",
       " ['j en ai peur', 'i m afraid so'],\n",
       " ['je suis tout seul', 'i m all alone'],\n",
       " ['je vais bien', 'i m all right'],\n",
       " ['je suis tout a toi', 'i m all yours'],\n",
       " ['je suis tout a vous', 'i m all yours'],\n",
       " ['je suis toute a toi', 'i m all yours'],\n",
       " ['je suis toute a vous', 'i m all yours'],\n",
       " ['je suis ambitieuse', 'i m ambitious'],\n",
       " ['je suis ambitieux', 'i m ambitious'],\n",
       " ['je suis un artiste', 'i m an artist'],\n",
       " ['je suis une artiste', 'i m an artist'],\n",
       " ['je suis orphelin', 'i m an orphan'],\n",
       " ['je suis attentive', 'i m attentive'],\n",
       " ['je suis attentif', 'i m attentive'],\n",
       " ['je suis disponible', 'i m available'],\n",
       " ['je suis belle', 'i m beautiful'],\n",
       " ['je suis beau', 'i m beautiful'],\n",
       " ['je suis egalement affaire', 'i m busy too'],\n",
       " ['je suis egalement affairee', 'i m busy too'],\n",
       " ['je suis egalement occupe', 'i m busy too'],\n",
       " ['je suis egalement occupee', 'i m busy too'],\n",
       " ['je suis preoccupe', 'i m concerned'],\n",
       " ['je suis preoccupee', 'i m concerned'],\n",
       " ['j ai confiance', 'i m confident'],\n",
       " ['me voila satisfait', 'i m contented'],\n",
       " ['me voila satisfaite', 'i m contented'],\n",
       " ['j en suis convaincu', 'i m convinced'],\n",
       " ['je suis deprime', 'i m depressed'],\n",
       " ['je suis desespere', 'i m desperate'],\n",
       " ['je suis desesperee', 'i m desperate'],\n",
       " ['je suis differente', 'i m different'],\n",
       " ['je suis degoute', 'i m disgusted'],\n",
       " ['je suis degoutee', 'i m disgusted'],\n",
       " ['je suis facile a vivre', 'i m easygoing'],\n",
       " ['je suis epuisee', 'i m exhausted'],\n",
       " ['je suis epuise', 'i m exhausted'],\n",
       " ['je suis vanne', 'i m exhausted'],\n",
       " ['je suis vannee', 'i m exhausted'],\n",
       " ['je suis fourbu', 'i m exhausted'],\n",
       " ['je suis creve', 'i m exhausted'],\n",
       " ['je suis crevee', 'i m exhausted'],\n",
       " ['je suis distrait', 'i m forgetful'],\n",
       " ['je suis distraite', 'i m forgetful'],\n",
       " ['je suis etourdi', 'i m forgetful'],\n",
       " ['je suis etourdie', 'i m forgetful'],\n",
       " ['je suis chez moi tom', 'i m home tom'],\n",
       " ['j ai mal au c ur', 'i m hung over'],\n",
       " ['je suis impatient', 'i m impatient'],\n",
       " ['je suis impatiente', 'i m impatient'],\n",
       " ['je suis important', 'i m important'],\n",
       " ['je suis impressionnee', 'i m impressed'],\n",
       " ['je suis impulsif', 'i m impulsive'],\n",
       " ['je suis impulsive', 'i m impulsive'],\n",
       " ['je suis a boston', 'i m in boston'],\n",
       " ['je suis en danger', 'i m in danger'],\n",
       " ['je suis intrigue', 'i m intrigued'],\n",
       " ['je suis intriguee', 'i m intrigued'],\n",
       " ['cela m intrigue', 'i m intrigued'],\n",
       " ['je suis juste paresseux', 'i m just lazy'],\n",
       " ['je suis juste paresseuse', 'i m just lazy'],\n",
       " ['j ecoute', 'i m listening'],\n",
       " ['je suis motivee', 'i m motivated'],\n",
       " ['je suis motive', 'i m motivated'],\n",
       " ['je ne suis pas un expert', 'i m no expert'],\n",
       " ['je ne suis pas flic', 'i m not a cop'],\n",
       " ['je ne fais pas partie de ses admirateurs', 'i m not a fan'],\n",
       " ['je ne fais pas partie de leurs admirateurs', 'i m not a fan'],\n",
       " ['je ne suis pas seul', 'i m not alone'],\n",
       " ['je ne suis pas seule', 'i m not alone'],\n",
       " ['je ne suis pas en colere !', 'i m not angry !'],\n",
       " ['je ne suis pas en colere', 'i m not angry'],\n",
       " ['je ne suis pas arme', 'i m not armed'],\n",
       " ['je ne suis pas armee', 'i m not armed'],\n",
       " ['je ne suis pas aveugle', 'i m not blind'],\n",
       " ['je ne suis pas fou', 'i m not crazy'],\n",
       " ['je ne suis pas folle', 'i m not crazy'],\n",
       " ['je ne suis pas en train de mourir', 'i m not dying'],\n",
       " ['je n y vais pas', 'i m not going'],\n",
       " ['je ne m y rends pas', 'i m not going'],\n",
       " ['je ne m en vais pas', 'i m not going'],\n",
       " ['je ne suis pas content', 'i m not happy'],\n",
       " ['je ne suis pas heureux', 'i m not happy'],\n",
       " ['je ne suis pas heureuse', 'i m not happy'],\n",
       " ['je ne suis pas contente', 'i m not happy'],\n",
       " ['je ne suis pas en train de mentir', 'i m not lying'],\n",
       " ['je ne suis pas naif', 'i m not naive'],\n",
       " ['je ne suis pas naive', 'i m not naive'],\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:38:51.803887Z",
     "start_time": "2024-03-26T13:38:51.792416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load(\"data/autoencoder_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:32.780272Z",
     "start_time": "2024-03-26T13:40:32.769288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1416,  0.8455, -0.2730,  ...,  2.1607, -0.4244,  0.3646],\n",
      "        [-0.0784,  0.6788, -2.1492,  ...,  1.2525,  1.3779, -1.0326],\n",
      "        [-0.2117, -2.4791,  1.2466,  ..., -1.0604, -0.5760,  0.3253],\n",
      "        ...,\n",
      "        [ 0.3594,  0.2941, -0.6878,  ...,  0.7309,  0.5011, -0.4817],\n",
      "        [-0.3278,  0.8841, -1.7787,  ...,  0.3559,  1.1744, -0.9567],\n",
      "        [-0.5141,  0.4928,  0.4809,  ..., -1.1174, -3.2719,  2.1448]])\n",
      "<built-in method requires_grad_ of Parameter object at 0x7fd0e5487270>\n",
      "Parameter containing:\n",
      "tensor([[ 0.3316, -0.1149, -0.1899,  ..., -0.3300, -0.0232, -0.1679],\n",
      "        [-0.1571, -0.2246, -0.0646,  ..., -0.1820,  0.0166, -0.1192],\n",
      "        [-0.1043, -0.1033,  0.0264,  ..., -0.3439, -0.3197,  0.0911],\n",
      "        ...,\n",
      "        [-0.4555, -0.2779,  0.3671,  ...,  0.0300, -0.0619, -0.1318],\n",
      "        [-0.1588,  0.2085,  0.3003,  ...,  0.3293, -0.0966,  0.1906],\n",
      "        [-0.1045, -0.0333,  0.0983,  ..., -0.2138, -0.1823,  0.1643]])\n",
      "<built-in method requires_grad_ of Parameter object at 0x7fd0e54872c0>\n",
      "Parameter containing:\n",
      "tensor([[ 0.1684,  0.2391, -0.0987,  ...,  0.1161,  0.0100,  0.0164],\n",
      "        [ 0.0581, -0.1712, -0.0335,  ...,  0.0709,  0.0858,  0.0896],\n",
      "        [-0.0774, -0.0748, -0.0925,  ..., -0.0387,  0.0622, -0.0355],\n",
      "        ...,\n",
      "        [ 0.0684,  0.0636,  0.0180,  ...,  0.0074,  0.0186, -0.1108],\n",
      "        [-0.0347,  0.0845,  0.1891,  ...,  0.0831, -0.1992,  0.2480],\n",
      "        [ 0.1063, -0.1079, -0.0716,  ..., -0.1725, -0.0105, -0.2049]])\n",
      "<built-in method requires_grad_ of Parameter object at 0x7fd0e54875e0>\n",
      "Parameter containing:\n",
      "tensor([-0.3196,  0.1736, -0.0570,  0.4243,  0.3299,  0.2807,  0.2819,  0.3941,\n",
      "        -0.1637, -0.1574, -0.2089, -0.0935,  0.2865,  0.1005, -0.1452, -0.2823,\n",
      "         0.0019, -0.1845,  0.1029,  0.0168, -0.0588, -0.0309, -0.1297,  0.0838,\n",
      "         0.3382,  0.3659, -0.0569,  0.1967,  0.5612, -0.0535,  0.1091,  0.1953,\n",
      "         0.5321,  0.5131,  0.0289,  0.0420, -0.0450,  0.3008,  0.1007,  0.3162,\n",
      "         0.3154,  0.1500,  0.2430,  0.4526,  0.2535, -0.2864,  0.1106,  0.0574,\n",
      "         0.2547,  0.3634,  0.1583,  0.4446,  0.0672,  0.4048,  0.1138, -0.0849,\n",
      "         0.2883, -0.1665, -0.1089,  0.6074, -0.1088,  0.3180,  0.1990, -0.0709,\n",
      "         0.1319,  0.1099,  0.1171,  0.0267, -0.2007, -0.3074,  0.0331, -0.1339,\n",
      "        -0.2958,  0.3588,  0.1961, -0.0843,  0.4782,  0.3362,  0.0861,  0.5647,\n",
      "         0.3937, -0.0718, -0.0972,  0.5957,  0.0566, -0.1035,  0.0634,  0.2178,\n",
      "         0.2567,  0.0012,  0.2122,  0.2814,  0.0763, -0.1710, -0.0144, -0.0116,\n",
      "         0.0175, -0.0101, -0.0141, -0.0981,  0.0917, -0.1657, -0.1882, -0.1629,\n",
      "         0.0585,  0.4703,  0.2023,  0.4315, -0.1015,  0.2301,  0.1665, -0.0285,\n",
      "         0.6651,  0.1896,  0.4298,  0.1577, -0.0444,  0.1207, -0.1528, -0.2413,\n",
      "        -0.1037,  0.2259, -0.1796, -0.0242,  0.3094, -0.0423,  0.1967,  0.0544,\n",
      "        -0.4724,  0.0199, -0.0710, -0.3183, -0.1354, -0.3693, -0.1934, -0.1130,\n",
      "        -0.3680, -0.3577, -0.5352, -0.3978, -0.7541, -0.5192, -0.0495, -0.2517,\n",
      "        -0.4982, -0.3833, -0.1895, -0.4964, -0.4640, -0.2286, -0.3362, -0.1592,\n",
      "        -0.1102, -0.2413,  0.0111, -0.2546, -0.1520, -0.4056, -0.3093, -0.3278,\n",
      "         0.0510, -0.6411, -0.7465, -0.2234, -0.6920, -0.0618, -0.2103, -0.2618,\n",
      "        -0.0689, -0.0112, -0.3504, -0.3622, -0.2199, -0.3045,  0.0272, -0.3308,\n",
      "        -0.1472, -0.2091, -0.0802, -0.4742, -0.1973, -0.1220, -0.1923,  0.0946,\n",
      "        -0.2961, -0.5447, -0.2297, -0.3209, -0.3593,  0.0115, -0.2658, -0.5878,\n",
      "         0.2447,  0.0654, -0.2155, -0.2675, -0.4276, -0.4454, -0.4649, -0.3945,\n",
      "        -0.3550, -0.0768, -0.1322, -0.3672,  0.0996, -0.3323, -0.1318, -0.3558,\n",
      "        -0.2878, -0.2352, -0.1024, -0.0387,  0.0215, -0.4085, -0.2556, -0.5119,\n",
      "        -0.1805, -0.0484, -0.2051,  0.0373, -0.5383, -0.2332,  0.0967, -0.1366,\n",
      "         0.1428, -0.0996, -0.3534, -0.3351,  0.0100, -0.5037, -0.4840, -0.6084,\n",
      "        -0.4427, -0.0888, -0.2368, -0.1576, -0.4545,  0.0086, -0.2981, -0.5488,\n",
      "        -0.0829, -0.2921, -0.2578, -0.0581, -0.2999, -0.4322, -0.3849, -0.2573,\n",
      "        -0.4938,  0.1363, -0.1888, -0.5130, -0.3016, -0.3665, -0.3016, -0.5312,\n",
      "         0.1473, -0.2271,  0.3744, -0.2210, -0.4407,  0.0934,  0.0971,  0.0602,\n",
      "        -0.0058,  0.1370,  0.0716, -0.0255,  0.3489, -0.2132, -0.0168,  0.1030,\n",
      "         0.3679,  0.1188, -0.4712, -0.1966,  0.0307,  0.2617, -0.1080, -0.0252,\n",
      "        -0.5196, -0.1351, -0.1758,  0.1819, -0.5246, -0.0023,  0.2562, -0.0997,\n",
      "         0.3885, -0.0919, -0.0800,  0.4242,  0.1169,  0.3837,  0.0128,  0.2061,\n",
      "        -0.1146,  0.0587,  0.2650,  0.1293, -0.1555, -0.2007, -0.0072, -0.1495,\n",
      "        -0.0094,  0.1299,  0.2273, -0.1524, -0.0168,  0.2592, -0.5152,  0.1758,\n",
      "        -0.4609, -0.2448, -0.0816, -0.4005, -0.1774,  0.3479,  0.3709,  0.2851,\n",
      "         0.3142,  0.4915, -0.4390, -0.1618, -0.2391, -0.0248,  0.1994, -0.1201,\n",
      "        -0.0368,  0.2471,  0.1903, -0.0471,  0.4767, -0.2968, -0.1874, -0.0985,\n",
      "        -0.3781,  0.1469, -0.1066,  0.1949, -0.2947,  0.0216, -0.1743, -0.0787,\n",
      "        -0.1655, -0.1493, -0.1535, -0.2433,  0.2380,  0.1368, -0.0503, -0.2595,\n",
      "        -0.0468, -0.3932,  0.2041,  0.0727, -0.3794,  0.1169,  0.2758, -0.1351,\n",
      "         0.3369,  0.4446,  0.3594,  0.1831, -0.0115,  0.0044,  0.3935, -0.4591,\n",
      "        -0.3094, -0.2424,  0.0151,  0.3949, -0.0560,  0.0838, -0.0443,  0.0859,\n",
      "         0.0324,  0.2310,  0.5010, -0.2103,  0.0483, -0.2461,  0.0482,  0.4027])\n",
      "<built-in method requires_grad_ of Parameter object at 0x7fd0e54874f0>\n",
      "Parameter containing:\n",
      "tensor([-2.3586e-01,  1.7742e-01,  1.1748e-02,  4.1019e-01,  3.0424e-01,\n",
      "         2.3205e-01,  2.6182e-01,  4.0640e-01, -2.9556e-01, -1.3761e-01,\n",
      "        -1.0009e-01, -1.9480e-01,  2.7167e-01, -1.2068e-02, -4.6060e-02,\n",
      "        -2.2794e-01, -6.9494e-02, -2.7324e-01,  2.5253e-01,  7.4837e-03,\n",
      "        -1.8149e-01, -5.1094e-02, -2.2188e-01,  4.7416e-02,  3.4633e-01,\n",
      "         3.1312e-01,  3.1383e-02,  9.3057e-02,  5.8185e-01, -4.6904e-02,\n",
      "         1.3746e-01,  1.9945e-01,  4.1717e-01,  5.7080e-01,  3.6915e-03,\n",
      "        -3.3871e-02, -1.1559e-01,  3.6328e-01,  1.8898e-02,  3.4591e-01,\n",
      "         2.4811e-01,  1.4970e-01,  1.2156e-01,  3.9118e-01,  1.8955e-01,\n",
      "        -2.2789e-01,  1.0327e-01, -5.1238e-02,  2.8949e-01,  2.6307e-01,\n",
      "         2.6355e-01,  4.6647e-01,  6.2986e-02,  2.4594e-01,  9.1214e-02,\n",
      "        -7.1795e-02,  2.8860e-01, -2.2717e-01, -1.0924e-01,  6.8003e-01,\n",
      "         8.4482e-03,  4.3586e-01,  2.0422e-01,  1.1464e-02,  1.2893e-01,\n",
      "         1.2371e-01,  2.3683e-01,  1.0695e-01, -2.4446e-01, -2.9821e-01,\n",
      "        -2.3437e-03, -1.0784e-01, -4.1188e-01,  2.6148e-01,  2.0770e-01,\n",
      "        -6.7501e-02,  5.6212e-01,  3.4211e-01,  7.6502e-02,  5.5911e-01,\n",
      "         4.5453e-01, -9.5554e-02,  6.2541e-02,  4.7116e-01,  8.8779e-03,\n",
      "        -1.3530e-01,  9.7080e-02,  1.2804e-01,  1.4088e-01,  3.0351e-03,\n",
      "         1.7801e-01,  1.1608e-01,  1.1393e-01, -2.4269e-01,  1.1113e-03,\n",
      "         6.9490e-02,  7.9809e-02,  6.1826e-03,  8.2543e-02, -1.4057e-01,\n",
      "         6.5186e-02, -1.2665e-01, -8.8505e-02, -1.1468e-01,  1.9122e-01,\n",
      "         4.1788e-01,  1.7094e-01,  2.6336e-01, -2.4376e-01,  2.4231e-01,\n",
      "         1.4213e-01,  3.6951e-02,  6.6583e-01,  1.5845e-01,  4.8101e-01,\n",
      "         7.4964e-02, -1.0423e-01,  1.3718e-01, -9.6786e-02, -1.8655e-01,\n",
      "        -1.1388e-01,  2.4387e-01, -1.7562e-01,  1.0394e-01,  3.7193e-01,\n",
      "        -1.2185e-01,  6.5966e-02,  9.3926e-02, -5.1677e-01,  1.4510e-01,\n",
      "        -1.2341e-02, -3.1720e-01, -1.7176e-01, -4.0588e-01, -2.1363e-01,\n",
      "        -1.4704e-01, -3.4580e-01, -2.8470e-01, -5.5928e-01, -4.2668e-01,\n",
      "        -6.9778e-01, -4.6453e-01,  1.8149e-02, -3.8711e-01, -4.1472e-01,\n",
      "        -4.5736e-01, -1.3918e-01, -4.8387e-01, -3.1904e-01, -2.4275e-01,\n",
      "        -3.3575e-01, -2.0022e-01, -1.6382e-01, -2.0878e-01,  1.0718e-01,\n",
      "        -1.9573e-01, -4.9966e-02, -3.3380e-01, -2.3428e-01, -3.3640e-01,\n",
      "         1.1293e-02, -7.7743e-01, -7.1260e-01, -1.9460e-01, -6.4483e-01,\n",
      "        -6.5284e-02, -8.1035e-02, -1.9821e-01, -2.2627e-02, -4.1229e-02,\n",
      "        -3.5048e-01, -3.5265e-01, -1.2262e-01, -3.3449e-01,  1.3967e-01,\n",
      "        -3.5995e-01, -2.0503e-01, -2.0563e-01,  6.5618e-02, -4.6696e-01,\n",
      "        -1.6831e-01, -1.2166e-01, -1.9976e-01,  1.3992e-01, -2.6294e-01,\n",
      "        -5.7681e-01, -3.1691e-01, -2.6281e-01, -4.3253e-01,  5.3921e-02,\n",
      "        -1.5749e-01, -6.1506e-01,  3.0785e-01,  1.1734e-01, -2.0492e-01,\n",
      "        -2.3654e-01, -4.6235e-01, -5.5260e-01, -4.5401e-01, -3.7612e-01,\n",
      "        -3.6361e-01, -7.5733e-03, -1.8903e-01, -4.5397e-01,  1.9849e-03,\n",
      "        -3.3794e-01, -1.3893e-01, -2.7486e-01, -3.8903e-01, -2.9914e-01,\n",
      "         9.4837e-03, -1.2975e-01, -5.7159e-02, -2.5960e-01, -1.8279e-01,\n",
      "        -4.7948e-01, -2.1247e-01, -5.9006e-02, -8.6986e-02, -1.2747e-01,\n",
      "        -5.1047e-01, -3.3230e-01,  1.1022e-01,  8.9054e-04,  1.2469e-01,\n",
      "        -5.5630e-02, -3.1102e-01, -3.5653e-01, -5.1276e-02, -5.4302e-01,\n",
      "        -4.1783e-01, -6.5629e-01, -2.8182e-01, -1.5117e-01, -2.5391e-01,\n",
      "        -6.3202e-02, -3.6575e-01,  3.5324e-02, -3.2443e-01, -5.9883e-01,\n",
      "        -3.2378e-02, -2.5654e-01, -2.1045e-01,  3.2893e-02, -3.7076e-01,\n",
      "        -4.8189e-01, -3.1012e-01, -3.8897e-01, -4.5567e-01,  8.0613e-02,\n",
      "        -3.3684e-01, -4.6839e-01, -3.6020e-01, -4.6758e-01, -3.3110e-01,\n",
      "        -4.2465e-01,  1.3715e-01, -3.1375e-01,  3.8515e-01, -1.3612e-01,\n",
      "        -3.0452e-01,  1.7375e-01,  2.5658e-04,  8.1697e-02,  5.6627e-02,\n",
      "        -3.1183e-01, -2.4507e-01, -1.9603e-01,  2.9294e-01, -2.4367e-01,\n",
      "         5.9642e-04,  3.1161e-01,  3.2796e-01,  1.7318e-01, -2.4204e-01,\n",
      "        -8.5172e-03, -1.4205e-01,  1.1998e-01, -2.4997e-01,  1.0672e-01,\n",
      "        -4.4858e-01,  1.1566e-01, -1.3404e-01, -6.3147e-04, -5.2670e-01,\n",
      "        -5.3691e-02,  2.7702e-01, -8.0662e-02,  3.0181e-01, -2.1609e-01,\n",
      "         2.7584e-02,  4.4129e-01,  4.1556e-01,  3.6724e-01,  1.0159e-02,\n",
      "         3.3198e-03, -1.2689e-01,  1.1500e-01,  1.6920e-01,  2.6677e-01,\n",
      "        -1.5247e-01, -1.6548e-01, -1.2498e-01, -1.8513e-01, -2.1280e-02,\n",
      "        -6.4928e-02,  9.4708e-02, -3.7592e-02,  7.9089e-02,  3.3622e-01,\n",
      "        -3.4134e-01,  1.3262e-01, -4.8050e-01, -2.6642e-01, -4.0072e-01,\n",
      "        -2.9889e-01, -1.9079e-01,  2.7317e-01,  3.8271e-01, -5.4659e-02,\n",
      "         1.5385e-01,  3.6507e-01, -5.4759e-01, -2.1937e-01,  2.6278e-02,\n",
      "        -2.9898e-01,  2.8846e-01, -4.8421e-02, -4.1757e-02,  1.0819e-01,\n",
      "         2.1877e-01,  3.8453e-01,  3.7086e-01, -3.8080e-01,  4.7872e-02,\n",
      "        -9.2838e-02, -7.3142e-01,  9.2164e-02, -2.8888e-01,  1.5353e-01,\n",
      "        -1.0833e-01, -1.8210e-01, -1.7275e-01, -5.8327e-02,  1.0977e-01,\n",
      "         2.3560e-02, -3.1121e-02, -1.7733e-02,  1.5756e-02, -7.1313e-02,\n",
      "        -1.3314e-01, -1.7308e-01,  9.7476e-02, -1.7663e-01,  2.8244e-01,\n",
      "        -9.1665e-02, -3.9041e-01, -2.7604e-02,  4.3605e-01, -1.4380e-02,\n",
      "         1.9410e-01,  2.4279e-01,  3.2050e-01,  1.0692e-01, -5.8711e-03,\n",
      "         1.5262e-03,  1.4049e-01, -3.7803e-01, -3.3327e-01, -1.1895e-01,\n",
      "         5.2508e-02,  1.8204e-01, -3.3468e-01,  1.0978e-01, -9.0519e-02,\n",
      "         1.1124e-01,  7.4180e-02,  2.3097e-01,  2.5383e-01, -6.5234e-01,\n",
      "        -1.4606e-01, -2.6087e-01,  1.3935e-01,  4.6039e-01])\n",
      "<built-in method requires_grad_ of Parameter object at 0x7fd0e54879a0>\n"
     ]
    }
   ],
   "source": [
    "for param in encoder.parameters():\n",
    "    print(param)\n",
    "    print(param.requires_grad_)\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:49.520820Z",
     "start_time": "2024-03-26T13:40:49.516592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(4601, 128)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:49.790686Z",
     "start_time": "2024-03-26T13:40:49.787648Z"
    }
   },
   "outputs": [],
   "source": [
    "pair = random.choice(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:50.191327Z",
     "start_time": "2024-03-26T13:40:50.187220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vous n etes pas aussi malin que moi', 'you re not as smart as me']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:50.542816Z",
     "start_time": "2024-03-26T13:40:50.537243Z"
    }
   },
   "outputs": [],
   "source": [
    "input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "\n",
    "input_tensor_2 = np.zeros((1, MAX_LENGTH), dtype = 'int')\n",
    "target_tensor_2 = np.zeros((1, MAX_LENGTH), dtype = 'int')\n",
    "\n",
    "input_tensor_2[:, :input_tensor.size(1)] = input_tensor[:, :]\n",
    "target_tensor_2[:, :target_tensor.size(1)] = target_tensor[:, :]\n",
    "\n",
    "input_tensor = torch.LongTensor(input_tensor_2).to(device)\n",
    "target_tensor = torch.LongTensor(target_tensor_2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:51.126002Z",
     "start_time": "2024-03-26T13:40:51.121547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vous n etes pas aussi malin que moi'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:51.359737Z",
     "start_time": "2024-03-26T13:40:51.355401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[116, 244, 213, 245, 928, 820, 901,  40,   1,   0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:51.526279Z",
     "start_time": "2024-03-26T13:40:51.521960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:51.709762Z",
     "start_time": "2024-03-26T13:40:51.705651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you re not as smart as me'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:51.877242Z",
     "start_time": "2024-03-26T13:40:51.872635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128,  77, 146, 889,  81, 889, 342,   1,   0,   0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:52.060179Z",
     "start_time": "2024-03-26T13:40:52.056440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:52.541602Z",
     "start_time": "2024-03-26T13:40:52.521499Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_hidden = encoder(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:52.743197Z",
     "start_time": "2024-03-26T13:40:52.739184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 128])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:52.926367Z",
     "start_time": "2024-03-26T13:40:52.922474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 128])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:53.341637Z",
     "start_time": "2024-03-26T13:40:53.326924Z"
    }
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "#         print(\"Attention\")\n",
    "#         print(\"Query Shape: \", query.shape)\n",
    "#         print(\"Keys Shape: \", keys.shape)\n",
    "#         print(\"Query Shape 2: \", self.Wa(query).shape)\n",
    "#         print(\"Keys Shape 2: \", self.Ua(keys).shape)\n",
    "#         print(\"Query + Keys Shape: \", (self.Wa(query) + self.Ua(keys)).shape)\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "#         print(\"Scores Shape: \", scores.shape)\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "#         print(\"Scores Shape 2: \", scores.shape)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "#         print(\"Weight Shape: \", scores.shape)\n",
    "        context = torch.bmm(weights, keys)\n",
    "#         print(\"Context Shape: \", scores.shape)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, embedding=None):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding = embedding\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "#         print(\"Decoder RNN\")\n",
    "#         print(\"Encoder Outputs Shape: \", encoder_outputs.shape)\n",
    "#         print(\"Encoder Hidden Shape: \", encoder_outputs.shape)\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "#         print(\"Decoder Input: \", decoder_input)\n",
    "#         print(\"Decoder Input Shape: \", decoder_input.shape)\n",
    "        decoder_hidden = encoder_hidden\n",
    "#         print(\"Decoder Hidden Shape: \", decoder_hidden.shape)\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "#             print(i)\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "#             print(\"Decoder Input Shape: \", decoder_input.shape)\n",
    "#             print(\"Decoder Output: \", decoder_output)\n",
    "#             print(\"Decoder Output Shape: \", decoder_output.shape)\n",
    "#             print(\"Attention Weights Shape: \", attn_weights.shape)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "#             print()\n",
    "                \n",
    "#         print(\"Loop done\")        \n",
    "#         print(\"Decoder Outputs: \", decoder_outputs)\n",
    "#         print(\"Decoder Outputs Len: \", len(decoder_outputs))\n",
    "        \n",
    "#         print(\"Attentions: \", attentions)\n",
    "#         print(\"Attentions Len: \", len(attentions))\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "#         print(\"Decoder Outputs Shape 2: \", decoder_outputs.shape)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "#         print(\"Decoder Outputs Shape 3: \", decoder_outputs.shape)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "#         print(\"Attentions Shape 2: \", attentions.shape)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "#         print(\"Forward Step\")\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "#         print(\"Embedded Shape: \", embedded.shape)\n",
    "#         print(\"Hidden Shape: \", hidden.shape)\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "#         print(\"Query Shape: \", query.shape)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "#         print(\"context Shape: \", context.shape)\n",
    "#         print(\"attn_weights Shape: \", attn_weights.shape)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "        \n",
    "#         print(\"input_gru Shape: \", input_gru.shape)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "#         print(\"output Shape: \", output.shape)\n",
    "#         print(\"hidden Shape: \", hidden.shape)\n",
    "        output = self.out(output)\n",
    "#         print(\"output Shape: \", output.shape)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:55.224708Z",
     "start_time": "2024-03-26T13:40:55.217978Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, embedding = encoder.embedding).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:40:55.929717Z",
     "start_time": "2024-03-26T13:40:55.911200Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden, \n",
    "                                                        target_tensor = target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T14:05:27.726634Z",
     "start_time": "2024-03-26T13:40:56.295987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 37s (- 24m 17s) (5 6%) 1.5110\n",
      "3m 8s (- 21m 57s) (10 12%) 0.7472\n",
      "4m 46s (- 20m 41s) (15 18%) 0.4939\n",
      "6m 17s (- 18m 52s) (20 25%) 0.3655\n",
      "7m 45s (- 17m 4s) (25 31%) 0.2913\n",
      "9m 13s (- 15m 22s) (30 37%) 0.2442\n",
      "10m 41s (- 13m 45s) (35 43%) 0.2105\n",
      "12m 10s (- 12m 10s) (40 50%) 0.1876\n",
      "13m 40s (- 10m 38s) (45 56%) 0.1695\n",
      "15m 13s (- 9m 7s) (50 62%) 0.1564\n",
      "16m 45s (- 7m 37s) (55 68%) 0.1447\n",
      "18m 16s (- 6m 5s) (60 75%) 0.1360\n",
      "19m 55s (- 4m 35s) (65 81%) 0.1282\n",
      "21m 28s (- 3m 4s) (70 87%) 0.1232\n",
      "22m 59s (- 1m 31s) (75 93%) 0.1166\n",
      "24m 31s (- 0m 0s) (80 100%) 0.1124\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set dropout layers to ``eval`` mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T14:09:36.728553Z",
     "start_time": "2024-03-26T14:09:36.658655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> tu es maline\n",
      "= you re clever\n",
      "< you re clever <EOS>\n",
      "\n",
      "> j en suis heureux\n",
      "= i m happy with it\n",
      "< i m happy happy with that <EOS>\n",
      "\n",
      "> je suis vraiment choque\n",
      "= i m really shocked\n",
      "< i m really shocked <EOS>\n",
      "\n",
      "> nous sommes tetus\n",
      "= we re stubborn\n",
      "< we re early <EOS>\n",
      "\n",
      "> nous allons toutes chez nous\n",
      "= we re all going home\n",
      "< we re all going home <EOS>\n",
      "\n",
      "> je suis desarme\n",
      "= i m powerless\n",
      "< i m excited <EOS>\n",
      "\n",
      "> je ne suis pas surpris\n",
      "= i m not surprised\n",
      "< i m not surprised <EOS>\n",
      "\n",
      "> tu n es pas citadine si ?\n",
      "= you re not a city girl are you ?\n",
      "< you re not a city girl are you ? <EOS>\n",
      "\n",
      "> nous n allons pas y parvenir\n",
      "= we re not going to make it\n",
      "< we re not gonna make it <EOS>\n",
      "\n",
      "> je sais nager\n",
      "= i m able to swim\n",
      "< i m able to swim now <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Attention\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step.\n",
    "\n",
    "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
    "displayed as a matrix. For a better viewing experience we will do the\n",
    "extra work of adding axes and labels:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:33:41.229342Z",
     "start_time": "2024-03-25T16:33:40.403756Z"
    }
   },
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
    "\n",
    "\n",
    "evaluateAndShowAttention('il n est pas aussi grand que son pere')\n",
    "\n",
    "evaluateAndShowAttention('je suis trop fatigue pour conduire')\n",
    "\n",
    "evaluateAndShowAttention('je suis desole si c est une question idiote')\n",
    "\n",
    "evaluateAndShowAttention('je suis reellement fiere de vous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "-  Try with a different dataset\n",
    "\n",
    "   -  Another language pair\n",
    "   -  Human → Machine (e.g. IOT commands)\n",
    "   -  Chat → Response\n",
    "   -  Question → Answer\n",
    "\n",
    "-  Replace the embeddings with pretrained word embeddings such as ``word2vec`` or\n",
    "   ``GloVe``\n",
    "-  Try with more layers, more hidden units, and more sentences. Compare\n",
    "   the training time and results.\n",
    "-  If you use a translation file where pairs have two of the same phrase\n",
    "   (``I am test \\t I am test``), you can use this as an autoencoder. Try\n",
    "   this:\n",
    "\n",
    "   -  Train as an autoencoder\n",
    "   -  Save only the Encoder network\n",
    "   -  Train a new Decoder for translation from there\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:35:00.833692Z",
     "start_time": "2024-03-25T16:35:00.822088Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save auto encoder model\n",
    "torch.save(encoder.state_dict(), \"data/autoencoder_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:36:21.859730Z",
     "start_time": "2024-03-25T16:36:21.848179Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_2 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "encoder_2.load_state_dict(torch.load(\"data/autoencoder_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:36:41.761068Z",
     "start_time": "2024-03-25T16:36:41.691409Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder_2, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
